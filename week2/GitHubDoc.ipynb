{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a83601b-95ac-4046-96de-12da1873c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-21 14:02:43--  https://github.com/alexeygrigorev/ai-bootcamp-codespace/raw/refs/heads/main/week1/docs.py\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/alexeygrigorev/ai-bootcamp-codespace/refs/heads/main/week1/docs.py [following]\n",
      "--2025-10-21 14:02:43--  https://raw.githubusercontent.com/alexeygrigorev/ai-bootcamp-codespace/refs/heads/main/week1/docs.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8583 (8.4K) [text/plain]\n",
      "Saving to: ‘docs.py’\n",
      "\n",
      "docs.py             100%[===================>]   8.38K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-10-21 14:02:43 (94.6 MB/s) - ‘docs.py’ saved [8583/8583]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/alexeygrigorev/ai-bootcamp-codespace/raw/refs/heads/main/week1/docs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f46f624-8b20-4ed2-b012-5627a7875cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "chunks = docs.chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc5234d-e5fc-4378-b297-8a084a570720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index\n",
    "index = Index(\n",
    "    text_fields=[\"content\",\"filename\",\"title\",\"description\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce685f4c-4341-4926-b407-56d7abcb166e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x73e2c2c74c20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603ab1e1-2a70-481c-b67f-d73f9c3e5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, TypedDict\n",
    "\n",
    "class SearchResult(TypedDict):\n",
    "    \"\"\"Represents a single search result entry.\"\"\"\n",
    "    start: int\n",
    "    content: str\n",
    "    title: str\n",
    "    description: str\n",
    "    filename: str\n",
    "\n",
    "\n",
    "def search(query: str) -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "    Search the index for documents matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[SearchResult]: A list of search results. Each result dictionary contains:\n",
    "            - start (int): The starting position or offset within the source file.\n",
    "            - content (str): A text excerpt or snippet containing the match.\n",
    "            - title (str): The title of the matched document.\n",
    "            - description (str): A short description of the document.\n",
    "            - filename (str): The path or name of the source file.\n",
    "    \"\"\"\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=5,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da89ef5-264b-4712-98ef-1f6b402bd23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index={}\n",
    "\n",
    "for doc in parsed_data:\n",
    "    filename = doc['filename']\n",
    "    file_index[filename] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56301ca8-fd1a-47f6-a053-a195653c889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def read_file(filename: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Retrieve the content of a file from the repository.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name or path of the file to read.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The file content as a string if the file exists;\n",
    "        otherwise, returns None.\n",
    "    \"\"\"\n",
    "    if filename in file_index:\n",
    "        return file_index[filename]['content']\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcd80aa1-f88c-4ffe-87e4-e837323b785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are an assistant that helps improve and generate high-quality documentation for the project.\n",
    "\n",
    "You have access to the following tools:\n",
    "- search — Use this to explore topics in depth. Make multiple search calls if needed to gather comprehensive information.\n",
    "- read_file — Use this when code snippets are missing or when you need to retrieve the full content of a file for context.\n",
    "\n",
    "If `read_file` cannot be used or the file content is unavailable, clearly state:\n",
    "> \"Unable to verify with read_file.\"\n",
    "\n",
    "When answering a question:\n",
    "1. Provide file references for all source materials.  \n",
    "   Use this format:  \n",
    "   [{filename}](https://github.com/evidentlyai/docs/blob/main/{filename})\n",
    "2. If the topic is covered in multiple documents, cite all relevant sources.\n",
    "3. Include code examples whenever they clarify or demonstrate the concept.\n",
    "4. Be concise, accurate, and helpful — focus on clarity and usability for developers.\n",
    "5. If documentation is missing or unclear, infer from context and note that explicitly.\n",
    "\n",
    "Example Citation:\n",
    "See the full implementation in [metrics/api_reference.md](https://github.com/evidentlyai/docs/blob/main/metrics/api_reference.md).\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f49b7d8-6455-44b3-afc2-964b6bd53b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.llm import OpenAIClient\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner\n",
    "from toyaikit.chat.runners import DisplayingRunnerCallback\n",
    "from toyaikit.tools import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7edd5d4-63e0-4466-90d5-bc92caf9650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools = Tools()\n",
    "\n",
    "agent_tools.add_tool(search)\n",
    "agent_tools.add_tool(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bce07edc-bef9-4476-908f-9be5265f42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=instructions,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35628fc4-b6ea-4220-848b-429587e28bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: what are the drift thresholds and how do I use them\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"drift thresholds\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"drift thresholds\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_EqVPRXfDdEnuCJbY9xGKsY7O', 'output': '[\\n  {\\n    \"start\": 3000,\\n    \"content\": \"cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\\\n</Info>\\\\n\\\\n## Data requirements\\\\n\\\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\\\n\\\\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\\\\n\\\\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\\\\n\\\\n<Info>\\\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\\\n</Info>\\\\n\\\\n## Report customization\\\\n\\\\nYou have multiple customization options.\\\\n\\\\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\\\n\\\\n**Change drift parameters.** You can modify how drift detection works:\\\\n\\\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\\\n\\\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\\\n\\\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\\\n\\\\n<Info>\\\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\\\n</Info>\\\\n\\\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\\\n\\\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in\",\\n    \"title\": \"Data Drift\",\\n    \"description\": \"Overview of the Data Drift Preset.\",\\n    \"filename\": \"metrics/preset_data_drift.mdx\"\\n  },\\n  {\\n    \"start\": 4000,\\n    \"content\": \"lumns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\\\n\\\\n**Change drift parameters.** You can modify how drift detection works:\\\\n\\\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\\\n\\\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\\\n\\\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\\\n\\\\n<Info>\\\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\\\n</Info>\\\\n\\\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\\\n\\\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift(\\\\\"prediction\\\\\")` to your Report so that you see the drift in this value in a separate widget.\\\\n\\\\n* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive stats and run Tests like detecting missing values. Data drift check drops nulls (and compares the distributions of non-empty features), so you may want to run these Tests separately.\\\\n\\\\n* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.\\\\n\\\\n<Info>\\\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\\\n</Info>\",\\n    \"title\": \"Data Drift\",\\n    \"description\": \"Overview of the Data Drift Preset.\",\\n    \"filename\": \"metrics/preset_data_drift.mdx\"\\n  },\\n  {\\n    \"start\": 7000,\\n    \"content\": \"ch as articles.\\\\n\\\\n<Tip>\\\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\\\n</Tip>\\\\n\\\\n\\\\n## Resources\\\\n\\\\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\\\\n\\\\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\\\\n\\\\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\\\n\\\\nAdditional links:\\\\n\\\\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\\\\n\\\\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\\\\n\\\\n* [\\\\\"My data drifted. What\\'s next?\\\\\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\\\\n\\\\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)\",\\n    \"title\": \"Data drift\",\\n    \"description\": \"How data drift detection works\",\\n    \"filename\": \"metrics/explainer_drift.mdx\"\\n  },\\n  {\\n    \"start\": 1000,\\n    \"content\": \"Target value, it will be evaluated together with other columns.\\\\n\\\\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\\\\n\\\\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\\\\n\\\\n![](/images/metrics/preset_data_drift-min.png)\\\\n\\\\nIf you choose to enable Tests, you will get an additional Test Suite view:\\\\n\\\\n![](/images/metrics/test_preset_data_drift-min.png)\\\\n\\\\n<Info>\\\\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\\\\n</Info>\\\\n\\\\n## Use case\\\\n\\\\nYou can evaluate data drift in different scenarios.\\\\n\\\\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\\\\n\\\\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\\\\n\\\\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\\\\n\\\\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\\\\n\\\\n<Info>\\\\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con\",\\n    \"title\": \"Data Drift\",\\n    \"description\": \"Overview of the Data Drift Preset.\",\\n    \"filename\": \"metrics/preset_data_drift.mdx\"\\n  },\\n  {\\n    \"start\": 0,\\n    \"content\": \"In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\\\\n\\\\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\\\\n\\\\n<Info>\\\\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\\\\n</Info>\\\\n\\\\n## How it works\\\\n\\\\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \\\\\"drift detected\\\\\" or \\\\\"not detected\\\\\" result.\\\\n\\\\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\\\\n\\\\n* column type: categorical, numerical, text data \\\\n\\\\n* the number of observations in the reference dataset\\\\n\\\\n* the number of unique values in the column (n\\\\\\\\_unique)\\\\n\\\\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\\\\n\\\\n## Data requirements\\\\n\\\\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\\\\n\\\\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\\\\n\\\\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\\\\n\\\\n<Note>\\\\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important \",\\n    \"title\": \"Data drift\",\\n    \"description\": \"How data drift detection works\",\\n    \"filename\": \"metrics/explainer_drift.mdx\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><h3>Drift Thresholds in Evidently</h3>\n",
       "<p><strong>Drift thresholds</strong> are critical parameters used in the drift detection process to determine when a statistically significant change has occurred in the data distribution. They allow you to customize how sensitive your drift detection is for individual columns or entire datasets.</p>\n",
       "<h4>Key Features of Drift Thresholds</h4>\n",
       "<ol>\n",
       "<li><p><strong>Change Detection Parameters</strong>:</p>\n",
       "<ul>\n",
       "<li>You can specify different parameters at both the dataset and column levels to fine-tune how drift detection operates.</li>\n",
       "<li>This includes specifying which columns to evaluate and how stringent the drift detection should be.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Customizable Methods</strong>:</p>\n",
       "<ul>\n",
       "<li>Evidently provides multiple drift detection methods such as Population Stability Index (PSI), Kullback-Leibler divergence, Jensen-Shannon distance, and Wasserstein distance. You can choose the appropriate method based on your data type and requirements.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Implementation</strong>:</p>\n",
       "<ul>\n",
       "<li>Drift detection methods can be customized further by implementing a custom drift method as a Python function if required.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Overall Dataset Drift</strong>:</p>\n",
       "<ul>\n",
       "<li>You can have an overall drift threshold where drift is deemed to have occurred if a certain percentage (the default is 50%) of the columns in the dataset show evidence of drift.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<h4>How to Use Drift Thresholds</h4>\n",
       "<ul>\n",
       "<li><p><strong>Define Your Datasets</strong>: Ensure you provide two datasets—the current one (to evaluate for drift) and the reference one (as a benchmark).</p>\n",
       "</li>\n",
       "<li><p><strong>Set Column Types</strong>: Explicitly map column types (numerical, categorical, text) to ensure accurate drift detection, though Evidently can auto-detect to some extent.</p>\n",
       "</li>\n",
       "<li><p><strong>Customize Parameters</strong>: Use the <code>columns</code> parameter to specify which features to monitor and adjust the drift detection thresholds and methods as necessary:</p>\n",
       "<pre><code class=\"language-python\">from evidently.metrics import DataDrift\n",
       "\n",
       "# Example of customizing drift detection\n",
       "drift = DataDrift(\n",
       "    current_data=current_dataset,\n",
       "    reference_data=reference_dataset,\n",
       "    columns=[&quot;feature1&quot;, &quot;feature2&quot;],\n",
       "    drift_params={&quot;method&quot;: &quot;PSI&quot;, &quot;threshold&quot;: 0.1}  # Customize your threshold\n",
       ")\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Monitor Results</strong>: Regularly check the results provided by the drift detection analysis:</p>\n",
       "<ul>\n",
       "<li>&quot;Drift detected&quot; or &quot;not detected&quot; messages based on the defined thresholds.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<h3>More Resources</h3>\n",
       "<ul>\n",
       "<li><p>For a more detailed overview of drift methods and parameters, refer to <a href=\"https://www.evidentlyai.com/docs/metrics/customize_data_drift\">Drift Customization</a>.</p>\n",
       "</li>\n",
       "<li><p>Explore examples of different drift tests and their applications through Evidently's blog on <a href=\"https://www.evidentlyai.com/blog/data-drift-detection-large-datasets\">data drift detection</a>.</p>\n",
       "</li>\n",
       "</ul>\n",
       "<p>By adjusting thresholds and methods as needed, you can tailor the drift detection capabilities to your specific needs, ensuring more accurate monitoring of your machine learning models.</p>\n",
       "<p>For more information, see the data drift documentation in <a href=\"https://github.com/evidentlyai/docs/blob/main/metrics/preset_data_drift.mdx\">Data Drift Overview</a>.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoopResult(new_messages=[{'role': 'developer', 'content': 'You are an assistant that helps improve and generate high-quality documentation for the project.\\n\\nYou have access to the following tools:\\n- search — Use this to explore topics in depth. Make multiple search calls if needed to gather comprehensive information.\\n- read_file — Use this when code snippets are missing or when you need to retrieve the full content of a file for context.\\n\\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\\n> \"Unable to verify with read_file.\"\\n\\nWhen answering a question:\\n1. Provide file references for all source materials.  \\n   Use this format:  \\n   [{filename}](https://github.com/evidentlyai/docs/blob/main/{filename})\\n2. If the topic is covered in multiple documents, cite all relevant sources.\\n3. Include code examples whenever they clarify or demonstrate the concept.\\n4. Be concise, accurate, and helpful — focus on clarity and usability for developers.\\n5. If documentation is missing or unclear, infer from context and note that explicitly.\\n\\nExample Citation:\\nSee the full implementation in [metrics/api_reference.md](https://github.com/evidentlyai/docs/blob/main/metrics/api_reference.md).'}, {'role': 'user', 'content': 'what are the drift thresholds and how do I use them'}, ResponseFunctionToolCall(arguments='{\"query\":\"drift thresholds\"}', call_id='call_EqVPRXfDdEnuCJbY9xGKsY7O', name='search', type='function_call', id='fc_0703245ecf609a0d0068f79d9f3f408190a71b55eb0fc13c3c', status='completed'), {'type': 'function_call_output', 'call_id': 'call_EqVPRXfDdEnuCJbY9xGKsY7O', 'output': '[\\n  {\\n    \"start\": 3000,\\n    \"content\": \"cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\\\n</Info>\\\\n\\\\n## Data requirements\\\\n\\\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\\\n\\\\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\\\\n\\\\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\\\\n\\\\n<Info>\\\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\\\n</Info>\\\\n\\\\n## Report customization\\\\n\\\\nYou have multiple customization options.\\\\n\\\\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\\\n\\\\n**Change drift parameters.** You can modify how drift detection works:\\\\n\\\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\\\n\\\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\\\n\\\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\\\n\\\\n<Info>\\\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\\\n</Info>\\\\n\\\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\\\n\\\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in\",\\n    \"title\": \"Data Drift\",\\n    \"description\": \"Overview of the Data Drift Preset.\",\\n    \"filename\": \"metrics/preset_data_drift.mdx\"\\n  },\\n  {\\n    \"start\": 4000,\\n    \"content\": \"lumns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\\\n\\\\n**Change drift parameters.** You can modify how drift detection works:\\\\n\\\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\\\n\\\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\\\n\\\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\\\n\\\\n<Info>\\\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\\\n</Info>\\\\n\\\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\\\n\\\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift(\\\\\"prediction\\\\\")` to your Report so that you see the drift in this value in a separate widget.\\\\n\\\\n* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive stats and run Tests like detecting missing values. Data drift check drops nulls (and compares the distributions of non-empty features), so you may want to run these Tests separately.\\\\n\\\\n* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.\\\\n\\\\n<Info>\\\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\\\n</Info>\",\\n    \"title\": \"Data Drift\",\\n    \"description\": \"Overview of the Data Drift Preset.\",\\n    \"filename\": \"metrics/preset_data_drift.mdx\"\\n  },\\n  {\\n    \"start\": 7000,\\n    \"content\": \"ch as articles.\\\\n\\\\n<Tip>\\\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\\\n</Tip>\\\\n\\\\n\\\\n## Resources\\\\n\\\\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\\\\n\\\\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\\\\n\\\\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\\\n\\\\nAdditional links:\\\\n\\\\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\\\\n\\\\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\\\\n\\\\n* [\\\\\"My data drifted. What\\'s next?\\\\\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\\\\n\\\\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)\",\\n    \"title\": \"Data drift\",\\n    \"description\": \"How data drift detection works\",\\n    \"filename\": \"metrics/explainer_drift.mdx\"\\n  },\\n  {\\n    \"start\": 1000,\\n    \"content\": \"Target value, it will be evaluated together with other columns.\\\\n\\\\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\\\\n\\\\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\\\\n\\\\n![](/images/metrics/preset_data_drift-min.png)\\\\n\\\\nIf you choose to enable Tests, you will get an additional Test Suite view:\\\\n\\\\n![](/images/metrics/test_preset_data_drift-min.png)\\\\n\\\\n<Info>\\\\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\\\\n</Info>\\\\n\\\\n## Use case\\\\n\\\\nYou can evaluate data drift in different scenarios.\\\\n\\\\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\\\\n\\\\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\\\\n\\\\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\\\\n\\\\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\\\\n\\\\n<Info>\\\\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con\",\\n    \"title\": \"Data Drift\",\\n    \"description\": \"Overview of the Data Drift Preset.\",\\n    \"filename\": \"metrics/preset_data_drift.mdx\"\\n  },\\n  {\\n    \"start\": 0,\\n    \"content\": \"In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\\\\n\\\\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\\\\n\\\\n<Info>\\\\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\\\\n</Info>\\\\n\\\\n## How it works\\\\n\\\\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \\\\\"drift detected\\\\\" or \\\\\"not detected\\\\\" result.\\\\n\\\\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\\\\n\\\\n* column type: categorical, numerical, text data \\\\n\\\\n* the number of observations in the reference dataset\\\\n\\\\n* the number of unique values in the column (n\\\\\\\\_unique)\\\\n\\\\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\\\\n\\\\n## Data requirements\\\\n\\\\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\\\\n\\\\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\\\\n\\\\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\\\\n\\\\n<Note>\\\\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important \",\\n    \"title\": \"Data drift\",\\n    \"description\": \"How data drift detection works\",\\n    \"filename\": \"metrics/explainer_drift.mdx\"\\n  }\\n]'}, ResponseOutputMessage(id='msg_0703245ecf609a0d0068f79da0a43081908d7b8d08a2a50dac', content=[ResponseOutputText(annotations=[], text='### Drift Thresholds in Evidently\\n\\n**Drift thresholds** are critical parameters used in the drift detection process to determine when a statistically significant change has occurred in the data distribution. They allow you to customize how sensitive your drift detection is for individual columns or entire datasets.\\n\\n#### Key Features of Drift Thresholds\\n\\n1. **Change Detection Parameters**: \\n   - You can specify different parameters at both the dataset and column levels to fine-tune how drift detection operates. \\n   - This includes specifying which columns to evaluate and how stringent the drift detection should be.\\n\\n2. **Customizable Methods**:\\n   - Evidently provides multiple drift detection methods such as Population Stability Index (PSI), Kullback-Leibler divergence, Jensen-Shannon distance, and Wasserstein distance. You can choose the appropriate method based on your data type and requirements.\\n\\n3. **Implementation**:\\n   - Drift detection methods can be customized further by implementing a custom drift method as a Python function if required.\\n\\n4. **Overall Dataset Drift**:\\n   - You can have an overall drift threshold where drift is deemed to have occurred if a certain percentage (the default is 50%) of the columns in the dataset show evidence of drift.\\n\\n#### How to Use Drift Thresholds\\n\\n- **Define Your Datasets**: Ensure you provide two datasets—the current one (to evaluate for drift) and the reference one (as a benchmark).\\n  \\n- **Set Column Types**: Explicitly map column types (numerical, categorical, text) to ensure accurate drift detection, though Evidently can auto-detect to some extent.\\n\\n- **Customize Parameters**: Use the `columns` parameter to specify which features to monitor and adjust the drift detection thresholds and methods as necessary:\\n  ```python\\n  from evidently.metrics import DataDrift\\n\\n  # Example of customizing drift detection\\n  drift = DataDrift(\\n      current_data=current_dataset,\\n      reference_data=reference_dataset,\\n      columns=[\"feature1\", \"feature2\"],\\n      drift_params={\"method\": \"PSI\", \"threshold\": 0.1}  # Customize your threshold\\n  )\\n  ```\\n\\n- **Monitor Results**: Regularly check the results provided by the drift detection analysis:\\n  - \"Drift detected\" or \"not detected\" messages based on the defined thresholds.\\n\\n### More Resources\\n\\n- For a more detailed overview of drift methods and parameters, refer to [Drift Customization](https://www.evidentlyai.com/docs/metrics/customize_data_drift).\\n\\n- Explore examples of different drift tests and their applications through Evidently\\'s blog on [data drift detection](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets).\\n\\nBy adjusting thresholds and methods as needed, you can tailor the drift detection capabilities to your specific needs, ensuring more accurate monitoring of your machine learning models. \\n\\nFor more information, see the data drift documentation in [Data Drift Overview](https://github.com/evidentlyai/docs/blob/main/metrics/preset_data_drift.mdx).', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], all_messages=[{'role': 'developer', 'content': 'You are an assistant that helps improve and generate high-quality documentation for the project.\\n\\nYou have access to the following tools:\\n- search — Use this to explore topics in depth. Make multiple search calls if needed to gather comprehensive information.\\n- read_file — Use this when code snippets are missing or when you need to retrieve the full content of a file for context.\\n\\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\\n> \"Unable to verify with read_file.\"\\n\\nWhen answering a question:\\n1. Provide file references for all source materials.  \\n   Use this format:  \\n   [{filename}](https://github.com/evidentlyai/docs/blob/main/{filename})\\n2. If the topic is covered in multiple documents, cite all relevant sources.\\n3. Include code examples whenever they clarify or demonstrate the concept.\\n4. Be concise, accurate, and helpful — focus on clarity and usability for developers.\\n5. If documentation is missing or unclear, infer from context and note that explicitly.\\n\\nExample Citation:\\nSee the full implementation in [metrics/api_reference.md](https://github.com/evidentlyai/docs/blob/main/metrics/api_reference.md).'}, {'role': 'user', 'content': 'what are the drift thresholds and how do I use them'}, ResponseFunctionToolCall(arguments='{\"query\":\"drift thresholds\"}', call_id='call_EqVPRXfDdEnuCJbY9xGKsY7O', name='search', type='function_call', id='fc_0703245ecf609a0d0068f79d9f3f408190a71b55eb0fc13c3c', status='completed'), {'type': 'function_call_output', 'call_id': 'call_EqVPRXfDdEnuCJbY9xGKsY7O', 'output': '[\\n  {\\n    \"start\": 3000,\\n    \"content\": \"cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\\\n</Info>\\\\n\\\\n## Data requirements\\\\n\\\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\\\n\\\\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\\\\n\\\\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\\\\n\\\\n<Info>\\\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\\\n</Info>\\\\n\\\\n## Report customization\\\\n\\\\nYou have multiple customization options.\\\\n\\\\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\\\n\\\\n**Change drift parameters.** You can modify how drift detection works:\\\\n\\\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\\\n\\\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\\\n\\\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\\\n\\\\n<Info>\\\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\\\n</Info>\\\\n\\\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\\\n\\\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in\",\\n    \"title\": \"Data Drift\",\\n    \"description\": \"Overview of the Data Drift Preset.\",\\n    \"filename\": \"metrics/preset_data_drift.mdx\"\\n  },\\n  {\\n    \"start\": 4000,\\n    \"content\": \"lumns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\\\n\\\\n**Change drift parameters.** You can modify how drift detection works:\\\\n\\\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\\\n\\\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\\\n\\\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\\\n\\\\n<Info>\\\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\\\n</Info>\\\\n\\\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\\\n\\\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift(\\\\\"prediction\\\\\")` to your Report so that you see the drift in this value in a separate widget.\\\\n\\\\n* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive stats and run Tests like detecting missing values. Data drift check drops nulls (and compares the distributions of non-empty features), so you may want to run these Tests separately.\\\\n\\\\n* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.\\\\n\\\\n<Info>\\\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\\\n</Info>\",\\n    \"title\": \"Data Drift\",\\n    \"description\": \"Overview of the Data Drift Preset.\",\\n    \"filename\": \"metrics/preset_data_drift.mdx\"\\n  },\\n  {\\n    \"start\": 7000,\\n    \"content\": \"ch as articles.\\\\n\\\\n<Tip>\\\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\\\n</Tip>\\\\n\\\\n\\\\n## Resources\\\\n\\\\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\\\\n\\\\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\\\\n\\\\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\\\n\\\\nAdditional links:\\\\n\\\\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\\\\n\\\\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\\\\n\\\\n* [\\\\\"My data drifted. What\\'s next?\\\\\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\\\\n\\\\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)\",\\n    \"title\": \"Data drift\",\\n    \"description\": \"How data drift detection works\",\\n    \"filename\": \"metrics/explainer_drift.mdx\"\\n  },\\n  {\\n    \"start\": 1000,\\n    \"content\": \"Target value, it will be evaluated together with other columns.\\\\n\\\\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\\\\n\\\\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\\\\n\\\\n![](/images/metrics/preset_data_drift-min.png)\\\\n\\\\nIf you choose to enable Tests, you will get an additional Test Suite view:\\\\n\\\\n![](/images/metrics/test_preset_data_drift-min.png)\\\\n\\\\n<Info>\\\\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\\\\n</Info>\\\\n\\\\n## Use case\\\\n\\\\nYou can evaluate data drift in different scenarios.\\\\n\\\\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\\\\n\\\\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\\\\n\\\\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\\\\n\\\\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\\\\n\\\\n<Info>\\\\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con\",\\n    \"title\": \"Data Drift\",\\n    \"description\": \"Overview of the Data Drift Preset.\",\\n    \"filename\": \"metrics/preset_data_drift.mdx\"\\n  },\\n  {\\n    \"start\": 0,\\n    \"content\": \"In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\\\\n\\\\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\\\\n\\\\n<Info>\\\\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\\\\n</Info>\\\\n\\\\n## How it works\\\\n\\\\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \\\\\"drift detected\\\\\" or \\\\\"not detected\\\\\" result.\\\\n\\\\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\\\\n\\\\n* column type: categorical, numerical, text data \\\\n\\\\n* the number of observations in the reference dataset\\\\n\\\\n* the number of unique values in the column (n\\\\\\\\_unique)\\\\n\\\\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\\\\n\\\\n## Data requirements\\\\n\\\\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\\\\n\\\\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\\\\n\\\\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\\\\n\\\\n<Note>\\\\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important \",\\n    \"title\": \"Data drift\",\\n    \"description\": \"How data drift detection works\",\\n    \"filename\": \"metrics/explainer_drift.mdx\"\\n  }\\n]'}, ResponseOutputMessage(id='msg_0703245ecf609a0d0068f79da0a43081908d7b8d08a2a50dac', content=[ResponseOutputText(annotations=[], text='### Drift Thresholds in Evidently\\n\\n**Drift thresholds** are critical parameters used in the drift detection process to determine when a statistically significant change has occurred in the data distribution. They allow you to customize how sensitive your drift detection is for individual columns or entire datasets.\\n\\n#### Key Features of Drift Thresholds\\n\\n1. **Change Detection Parameters**: \\n   - You can specify different parameters at both the dataset and column levels to fine-tune how drift detection operates. \\n   - This includes specifying which columns to evaluate and how stringent the drift detection should be.\\n\\n2. **Customizable Methods**:\\n   - Evidently provides multiple drift detection methods such as Population Stability Index (PSI), Kullback-Leibler divergence, Jensen-Shannon distance, and Wasserstein distance. You can choose the appropriate method based on your data type and requirements.\\n\\n3. **Implementation**:\\n   - Drift detection methods can be customized further by implementing a custom drift method as a Python function if required.\\n\\n4. **Overall Dataset Drift**:\\n   - You can have an overall drift threshold where drift is deemed to have occurred if a certain percentage (the default is 50%) of the columns in the dataset show evidence of drift.\\n\\n#### How to Use Drift Thresholds\\n\\n- **Define Your Datasets**: Ensure you provide two datasets—the current one (to evaluate for drift) and the reference one (as a benchmark).\\n  \\n- **Set Column Types**: Explicitly map column types (numerical, categorical, text) to ensure accurate drift detection, though Evidently can auto-detect to some extent.\\n\\n- **Customize Parameters**: Use the `columns` parameter to specify which features to monitor and adjust the drift detection thresholds and methods as necessary:\\n  ```python\\n  from evidently.metrics import DataDrift\\n\\n  # Example of customizing drift detection\\n  drift = DataDrift(\\n      current_data=current_dataset,\\n      reference_data=reference_dataset,\\n      columns=[\"feature1\", \"feature2\"],\\n      drift_params={\"method\": \"PSI\", \"threshold\": 0.1}  # Customize your threshold\\n  )\\n  ```\\n\\n- **Monitor Results**: Regularly check the results provided by the drift detection analysis:\\n  - \"Drift detected\" or \"not detected\" messages based on the defined thresholds.\\n\\n### More Resources\\n\\n- For a more detailed overview of drift methods and parameters, refer to [Drift Customization](https://www.evidentlyai.com/docs/metrics/customize_data_drift).\\n\\n- Explore examples of different drift tests and their applications through Evidently\\'s blog on [data drift detection](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets).\\n\\nBy adjusting thresholds and methods as needed, you can tailor the drift detection capabilities to your specific needs, ensuring more accurate monitoring of your machine learning models. \\n\\nFor more information, see the data drift documentation in [Data Drift Overview](https://github.com/evidentlyai/docs/blob/main/metrics/preset_data_drift.mdx).', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], tokens=TokenUsage(input_tokens=3277, output_tokens=633, total_tokens=3910), cost=CostInfo(input_cost=0.00049155, output_cost=0.00037979999999999996, total_cost=0.0008713499999999999), last_message='### Drift Thresholds in Evidently\\n\\n**Drift thresholds** are critical parameters used in the drift detection process to determine when a statistically significant change has occurred in the data distribution. They allow you to customize how sensitive your drift detection is for individual columns or entire datasets.\\n\\n#### Key Features of Drift Thresholds\\n\\n1. **Change Detection Parameters**: \\n   - You can specify different parameters at both the dataset and column levels to fine-tune how drift detection operates. \\n   - This includes specifying which columns to evaluate and how stringent the drift detection should be.\\n\\n2. **Customizable Methods**:\\n   - Evidently provides multiple drift detection methods such as Population Stability Index (PSI), Kullback-Leibler divergence, Jensen-Shannon distance, and Wasserstein distance. You can choose the appropriate method based on your data type and requirements.\\n\\n3. **Implementation**:\\n   - Drift detection methods can be customized further by implementing a custom drift method as a Python function if required.\\n\\n4. **Overall Dataset Drift**:\\n   - You can have an overall drift threshold where drift is deemed to have occurred if a certain percentage (the default is 50%) of the columns in the dataset show evidence of drift.\\n\\n#### How to Use Drift Thresholds\\n\\n- **Define Your Datasets**: Ensure you provide two datasets—the current one (to evaluate for drift) and the reference one (as a benchmark).\\n  \\n- **Set Column Types**: Explicitly map column types (numerical, categorical, text) to ensure accurate drift detection, though Evidently can auto-detect to some extent.\\n\\n- **Customize Parameters**: Use the `columns` parameter to specify which features to monitor and adjust the drift detection thresholds and methods as necessary:\\n  ```python\\n  from evidently.metrics import DataDrift\\n\\n  # Example of customizing drift detection\\n  drift = DataDrift(\\n      current_data=current_dataset,\\n      reference_data=reference_dataset,\\n      columns=[\"feature1\", \"feature2\"],\\n      drift_params={\"method\": \"PSI\", \"threshold\": 0.1}  # Customize your threshold\\n  )\\n  ```\\n\\n- **Monitor Results**: Regularly check the results provided by the drift detection analysis:\\n  - \"Drift detected\" or \"not detected\" messages based on the defined thresholds.\\n\\n### More Resources\\n\\n- For a more detailed overview of drift methods and parameters, refer to [Drift Customization](https://www.evidentlyai.com/docs/metrics/customize_data_drift).\\n\\n- Explore examples of different drift tests and their applications through Evidently\\'s blog on [data drift detection](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets).\\n\\nBy adjusting thresholds and methods as needed, you can tailor the drift detection capabilities to your specific needs, ensuring more accurate monitoring of your machine learning models. \\n\\nFor more information, see the data drift documentation in [Data Drift Overview](https://github.com/evidentlyai/docs/blob/main/metrics/preset_data_drift.mdx).')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605f579-f961-44c7-a07a-cb1c68bbb62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
