{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40818013-5172-41b0-b044-9c4242597ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool, Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf8138f-87f4-4dd9-8794-f696de05d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ae503c-66d0-4547-9745-cd61fe63f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af14309-c263-431c-92a5-286c0d05ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIAgentsSDKRunner\n",
    "\n",
    "chat_interface = IPythonChatInterface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb2d99a1-4750-4420-bb1a-d9ee3158b263",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assistant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m runner = OpenAIAgentsSDKRunner(\n\u001b[32m      2\u001b[39m     chat_interface = chat_interface,\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     agent = \u001b[43massistant\u001b[49m\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'assistant' is not defined"
     ]
    }
   ],
   "source": [
    "runner = OpenAIAgentsSDKRunner(\n",
    "    chat_interface = chat_interface,\n",
    "    agent = assistant\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505951c-501c-41fb-abd2-202641fbe328",
   "metadata": {},
   "source": [
    "### Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b797c61-852d-40f1-ab1c-17d2893a4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec1b42cb-04e9-4abd-a997-e3dd4992d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles = youtube.fetch_transcript_cached('vK_SxyqIfwk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaa19c6f-08c2-4b45-b450-7b4999b612f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_youtube_transcript(video_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the transcript of a YouTube video and converts it into a subtitle-formatted string.\n",
    "\n",
    "    Args:\n",
    "        video_id (str): The unique YouTube video ID.\n",
    "\n",
    "    Returns:\n",
    "        str: The subtitles generated from the video's transcript.\n",
    "    \"\"\"\n",
    "    return youtube.fetch_transcript_cached(video_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ab76317-21cc-487b-b3fe-5c00e7169f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_instructions = \"\"\"\n",
    "You're a helpful assistant that summarizes YouTube videos\n",
    "\"\"\"\n",
    "\n",
    "tools = [\n",
    "    function_tool(fetch_youtube_transcript)\n",
    "]\n",
    "\n",
    "summary_agent = Agent(\n",
    "    name='summary agent',\n",
    "    tools=tools,\n",
    "    handoff_description=\"Whenever the user needs a summary of the video\",\n",
    "    instructions=summary_instructions,\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48225478-9284-46b1-908b-0521012f78c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "runner = OpenAIAgentsSDKRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=summary_agent\n",
    ")\n",
    "\n",
    "await runner.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91790e13-0740-4c92-9ac3-4d3a0a5b1efa",
   "metadata": {},
   "source": [
    "### search agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0495c42-531c-4bc9-acf1-93af75aaac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import AppendableIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11c080dd-1e37-4c2e-9d13-eed940b4bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83cfe6b4-291c-4198-89d5-6864c101e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = AppendableIndex(text_fields=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3f709a6-68c3-4693-8025-88972043c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import AppendableIndex\n",
    "import docs\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class SearchTools:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.index = AppendableIndex(text_fields=['content'])\n",
    "\n",
    "    def search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search the index for documents matching a query string.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of search result dictionaries.\n",
    "        \"\"\"\n",
    "        return self.index.search(query, num_results=5)\n",
    "\n",
    "    def index_content(self, video_id: str, content: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Index text content from a given source into the search index.\n",
    "\n",
    "        Args:\n",
    "            video_id (str): video_id or link to the web page.\n",
    "            content (str): transcript for the provided video_id or None if the transcript needs to be downloaded\n",
    "\n",
    "        Returns:\n",
    "            str: \"SUCCESS\" upon successful indexing.\n",
    "        \"\"\"\n",
    "        if content is None:\n",
    "            content = fetch_youtube_transcript(video_id)\n",
    "        chunks = docs.sliding_window(content, size=3000, step=1500)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk[\"video_id\"] = video_id\n",
    "            self.index.append(chunk)\n",
    "\n",
    "        return \"SUCCESS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78a54348-a48e-4ba3-ba0e-898c8781c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tools= SearchTools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c451c2b-2274-403d-950b-3b746173e3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tools.index_content('vK_SxyqIfwk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "09c9c3c2-2931-48ca-a0a8-db0cc8bc8d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'content': \"0:00 Hey everyone, welcome to our event. This\\n0:02 event is brought to you by data talks\\n0:03 club which is a community of people who\\n0:05 love data. We have weekly events today.\\n0:08 Uh this is one of such events. Um if you\\n0:11 want to find out more about the events\\n0:13 we have, there is a link in the\\n0:14 description. Um so click on that link,\\n0:16 check it out right now. We actually have\\n0:19 quite a few events in our pipeline, but\\n0:21 we need to put them on the website. Uh\\n0:24 but keep an eye on it anyways. Um so we\\n0:27 will put um them. you'll see them. And\\n0:30 then don't forget to subscribe to our\\n0:32 YouTube channel. So this is the most um\\n0:35 reliable way of getting notified when\\n0:37 our stream starts. And last but not\\n0:40 least, do not forget to join our um data\\n0:43 community where you can hang out with\\n0:45 data with other data enthusiasts. During\\n0:48 today's interview, you can ask any\\n0:49 question you want. There is a pinned\\n0:51 link in the live chat. Click on that\\n0:53 link, ask your questions and we'll be\\n0:56 covering your questions during the\\n0:58 interview. So that's the usual\\n1:01 introduction I do.\\n1:04 Also, I'm a bit sleepy. Um, but I hope\\n1:07 it goes well.\\n1:10 So I guess if you're ready, I have the\\n1:13 questions prepared\\n1:16 in front of me.\\n1:17 Yeah,\\n1:18 if you're ready, we can start.\\n1:20 Yeah, sounds good. Ready? Um, today on\\n1:24 the podcast we are joined by Aishwara.\\n1:26 Do you pronounce your name correctly?\\n1:28 Yes, that is right.\\n1:29 Yeah, good. A machine learning engineer\\n1:32 at Vimo, formerly part of Tesla's\\n1:34 autopilot AI team and a Cambridge Melon\\n1:36 University Alumni. Aishwara has worked\\n1:39 across some of the toughest applied AI\\n1:42 problems. Financial recommendation\\n1:44 systems at Morgan Stanley, multimodel\\n1:46 research at CMU, perception and video\\n1:49 understanding and Tesla and now gesture\\n1:51 and pedestrial semantics at IMA. She has\\n1:54 also contributed to AI for social good\\n1:56 including a malaria mapping project in\\n1:58 Africa that achieved real world impact\\n2:00 at scale.\\n2:02 Welcome to this event.\\n2:06 Hi, thank you for having me. Yeah,\\n2:09 that's um quite a nice bio. Um\\n2:12 especially um I don't know probably\\n2:16 um what you do now uh at and Tesla\\n2:19 is challenging but for me also Morgan\\n2:22 Stanley sounds very challenging cuz um I\\n2:26 worked a little near high frequency\\n2:29 trading. So I wasn't actually working on\\n2:32 the system that we were doing tra uh\\n2:33 high frequency trading but we were doing\\n2:36 some um\\n2:38 how to say analytics on top of this\\n2:40 data. It was huge. So probably quite\\n2:44 challenging. Um so can you tell us uh I\\n2:48 just outlined your uh career journey but\\n2:51 can you tell us more about this? Um\\n2:55 yeah uh so I I think uh like you\\n2:58 mentioned at Morgan Stanley it was a lot\\n3:01 of data. So I was a big data engineer at\\n3:03 Morgan Stanley and I basically did that\\n3:06 handle all this huge amounts of data and\\n3:10 I was there w\",\n",
       "  'video_id': 'vK_SxyqIfwk'},\n",
       " {'start': 1500,\n",
       "  'content': \"t.\\n1:29 Yeah, good. A machine learning engineer\\n1:32 at Vimo, formerly part of Tesla's\\n1:34 autopilot AI team and a Cambridge Melon\\n1:36 University Alumni. Aishwara has worked\\n1:39 across some of the toughest applied AI\\n1:42 problems. Financial recommendation\\n1:44 systems at Morgan Stanley, multimodel\\n1:46 research at CMU, perception and video\\n1:49 understanding and Tesla and now gesture\\n1:51 and pedestrial semantics at IMA. She has\\n1:54 also contributed to AI for social good\\n1:56 including a malaria mapping project in\\n1:58 Africa that achieved real world impact\\n2:00 at scale.\\n2:02 Welcome to this event.\\n2:06 Hi, thank you for having me. Yeah,\\n2:09 that's um quite a nice bio. Um\\n2:12 especially um I don't know probably\\n2:16 um what you do now uh at and Tesla\\n2:19 is challenging but for me also Morgan\\n2:22 Stanley sounds very challenging cuz um I\\n2:26 worked a little near high frequency\\n2:29 trading. So I wasn't actually working on\\n2:32 the system that we were doing tra uh\\n2:33 high frequency trading but we were doing\\n2:36 some um\\n2:38 how to say analytics on top of this\\n2:40 data. It was huge. So probably quite\\n2:44 challenging. Um so can you tell us uh I\\n2:48 just outlined your uh career journey but\\n2:51 can you tell us more about this? Um\\n2:55 yeah uh so I I think uh like you\\n2:58 mentioned at Morgan Stanley it was a lot\\n3:01 of data. So I was a big data engineer at\\n3:03 Morgan Stanley and I basically did that\\n3:06 handle all this huge amounts of data and\\n3:10 I was there when Morgan Stanley you know\\n3:12 they were doing this acquisition of\\n3:14 Erade. So we had like a lot more data\\n3:16 coming in. So my role there was you know\\n3:19 handling all this data. How do we\\n3:21 connect the different dots? how do we\\n3:22 analyze them together? And from there I\\n3:26 uh like realized that you know there's\\n3:29 so much of data that it has so much of\\n3:30 value that we don't need to do all the\\n3:33 things that we do manually and that was\\n3:35 back I guess in 2018 and the whole AI\\n3:39 domain the AI bubble hadn't like formed\\n3:41 yet but you know it was getting there\\n3:43 people were realizing how important it\\n3:45 was and uh finance was one of the last\\n3:48 fields to take on uh the machine\\n3:50 learning aspect the AI aspect so we were\\n3:53 onboarding systems And that's when I\\n3:55 decided to like you know get a hand at\\n3:58 it and like try some of like begin with\\n4:00 some of the smaller systems like you\\n4:02 know recommendation engines and uh stuff\\n4:05 that was already well known in the uh AI\\n4:09 domain. So I started off as that and\\n4:12 then we decided to get a bit more uh I\\n4:16 guess researchy with that. We tried out\\n4:18 like you know graph neural networks\\n4:20 which were some of the more complicated\\n4:22 topics at that point and that's when I\\n4:24 realized that oh it's you know there's\\n4:26 so much to be learned here and there's\\n4:28 so many this this field is so vast so\\n4:30 that's when I decided to like uh pursue\\n4:33 a masters\\n4:34 and I like de\",\n",
       "  'video_id': 'vK_SxyqIfwk'},\n",
       " {'start': 3000,\n",
       "  'content': \"hen Morgan Stanley you know\\n3:12 they were doing this acquisition of\\n3:14 Erade. So we had like a lot more data\\n3:16 coming in. So my role there was you know\\n3:19 handling all this data. How do we\\n3:21 connect the different dots? how do we\\n3:22 analyze them together? And from there I\\n3:26 uh like realized that you know there's\\n3:29 so much of data that it has so much of\\n3:30 value that we don't need to do all the\\n3:33 things that we do manually and that was\\n3:35 back I guess in 2018 and the whole AI\\n3:39 domain the AI bubble hadn't like formed\\n3:41 yet but you know it was getting there\\n3:43 people were realizing how important it\\n3:45 was and uh finance was one of the last\\n3:48 fields to take on uh the machine\\n3:50 learning aspect the AI aspect so we were\\n3:53 onboarding systems And that's when I\\n3:55 decided to like you know get a hand at\\n3:58 it and like try some of like begin with\\n4:00 some of the smaller systems like you\\n4:02 know recommendation engines and uh stuff\\n4:05 that was already well known in the uh AI\\n4:09 domain. So I started off as that and\\n4:12 then we decided to get a bit more uh I\\n4:16 guess researchy with that. We tried out\\n4:18 like you know graph neural networks\\n4:20 which were some of the more complicated\\n4:22 topics at that point and that's when I\\n4:24 realized that oh it's you know there's\\n4:26 so much to be learned here and there's\\n4:28 so many this this field is so vast so\\n4:30 that's when I decided to like uh pursue\\n4:33 a masters\\n4:34 and I like decided to join Kangi Melon\\n4:37 University and my program it was like\\n4:41 sorry it was a mix of data science and\\n4:44 uh machine learning so I had like I\\n4:46 could draw upon both my experiences and\\n4:48 what I wanted to learn and where I\\n4:50 wanted to get at and dur during CMU I\\n4:53 was more like uh inclined towards\\n4:56 projects that involved a lot of computer\\n4:58 vision. So I was involved in this\\n5:00 project project for like navigational\\n5:03 app for blind people. Uh so it's called\\n5:06 like AI guide dog. So it it like takes\\n5:09 in the world and navigates uh people\\n5:11 without vision. And from there, you\\n5:14 know, I got into Tesla because it's also\\n5:16 similar like computer vision domain and\\n5:18 navigation related stuff. Uh and that's\\n5:21 where my self-driving journey began. And\\n5:23 then from Tesla to Wayob, it's like a\\n5:26 similar domain but uh different kinds of\\n5:28 products. Uh like uh some different like\\n5:31 differences are there. Uh and yeah, uh\\n5:34 that's where I am at Whimo right now. um\\n5:37 in the self-driving domain started from\\n5:39 finance uh reached in a different domain\\n5:42 but yeah\\n5:43 that's that's an interesting journey uh\\n5:45 this app that you developed for blind\\n5:48 people\\n5:49 um can you tell us more about it like\\n5:52 how does it work uh is it uh like they\\n5:55 hold their phone and then it tells uh\\n5:57 where to go or describes things\\n6:01 yeah sorry\\n6:03 so yeah for this uh app like the goal\\n6:06 was that you know uh people without\\n\",\n",
       "  'video_id': 'vK_SxyqIfwk'},\n",
       " {'start': 4500,\n",
       "  'content': \"cided to join Kangi Melon\\n4:37 University and my program it was like\\n4:41 sorry it was a mix of data science and\\n4:44 uh machine learning so I had like I\\n4:46 could draw upon both my experiences and\\n4:48 what I wanted to learn and where I\\n4:50 wanted to get at and dur during CMU I\\n4:53 was more like uh inclined towards\\n4:56 projects that involved a lot of computer\\n4:58 vision. So I was involved in this\\n5:00 project project for like navigational\\n5:03 app for blind people. Uh so it's called\\n5:06 like AI guide dog. So it it like takes\\n5:09 in the world and navigates uh people\\n5:11 without vision. And from there, you\\n5:14 know, I got into Tesla because it's also\\n5:16 similar like computer vision domain and\\n5:18 navigation related stuff. Uh and that's\\n5:21 where my self-driving journey began. And\\n5:23 then from Tesla to Wayob, it's like a\\n5:26 similar domain but uh different kinds of\\n5:28 products. Uh like uh some different like\\n5:31 differences are there. Uh and yeah, uh\\n5:34 that's where I am at Whimo right now. um\\n5:37 in the self-driving domain started from\\n5:39 finance uh reached in a different domain\\n5:42 but yeah\\n5:43 that's that's an interesting journey uh\\n5:45 this app that you developed for blind\\n5:48 people\\n5:49 um can you tell us more about it like\\n5:52 how does it work uh is it uh like they\\n5:55 hold their phone and then it tells uh\\n5:57 where to go or describes things\\n6:01 yeah sorry\\n6:03 so yeah for this uh app like the goal\\n6:06 was that you know uh people without\\n6:08 vision they should be able to navigate\\n6:10 the world just as cited people do. So\\n6:13 this app is basically their eyes. So it\\n6:16 you just like hang it around your neck\\n6:18 and you walk with it and it gets a world\\n6:21 view of what is in front of you and then\\n6:24 uh via like live audio instructions. It\\n6:27 tells you like you know keep walking\\n6:29 straight and uh if you've entered a\\n6:31 destination based on that you know take\\n6:33 left or right or stop at a traffic\\n6:35 signal or there's like you know\\n6:36 pedestrian crossings so it gives you\\n6:39 instructions via audio.\\n6:41 Interesting. And is it something you did\\n6:44 just as a pet project? Was it a part of\\n6:46 a company? Was it AI for social good\\n6:49 project? How did you\\n6:52 um get involved?\\n6:53 Yeah. So uh so my program has this uh\\n6:57 thing called the capstone project. So\\n6:58 every year like you either pair up with\\n7:01 a professor or someone from the industry\\n7:03 who already has a very interesting\\n7:05 project and then you work on it. So this\\n7:08 project was from an alumni uh of CMU and\\n7:11 he currently works at Pinterest. Uh but\\n7:14 he started this whole project and uh\\n7:16 this was like the third iteration where\\n7:18 you know he worked with two groups of\\n7:19 folks before and then uh my team on\\n7:22 boarded on it. Mhm. Okay. But was it um\\n7:26 like a community AI social good project\\n7:29 or was it a company?\\n7:32 Um I would say it was like more of a\\n7:35 volunteer project. So uh it's it's just\\n7:\",\n",
       "  'video_id': 'vK_SxyqIfwk'},\n",
       " {'start': 10500,\n",
       "  'content': \"\\n10:37 this, right?\\n10:38 Yeah, we that that's the hope like you\\n10:40 know, they don't need like uh a person\\n10:43 or some rely on someone. They can just\\n10:45 have their app and that can be their\\n10:47 guide for the world. Yeah. Mhm. And also\\n10:50 with this um VR glasses um you probably\\n10:55 heard like I think Meta has them, some\\n10:57 other companies have them. So you put\\n10:59 them in your eyes and they uh have\\n11:02 cameras, right? And the cameras they\\n11:05 have broader,\\n11:08 how to say vision than mobile phones.\\n11:12 Yeah. Yes.\\n11:12 Right. So maybe\\n11:15 Yeah. Maybe this is something that um\\n11:18 future alumni can work on, right?\\n11:22 Or like\\n11:27 cars, right? Not necessarily cameras.\\n11:30 Yeah, that's like, you know, those are\\n11:32 the things that we're trying to work\\n11:33 around because it needs to be cost\\n11:35 efficient and you know, we can't afford\\n11:36 to put like lighters and stuff. So, a\\n11:38 mobile phone, everyone has it like in\\n11:41 today's world. So, we're trying to fit\\n11:42 it on what everyone has. How expensive\\n11:46 is uh uh lit lighter?\\n11:50 I think it depends on the quality like\\n11:52 uh you can get from somewhere like\\n11:55 really cheap to extremely high-end. Uh\\n11:58 so yeah, I guess the\\n12:00 pronounce it lighter. LAR\\n12:03 it's LAR. Yeah,\\n12:04 LAR. So this is I know radar. So radar\\n12:08 emits uh uh radio frequencies and one\\n12:12 waits back to the frequency to come back\\n12:14 to the wave to come back right and based\\n12:17 on that uh the the radar can estimate if\\n12:21 there's something\\n12:24 uh and if it's moving\\n12:26 uh and so on. Um whales do this right?\\n12:29 So they make sound\\n12:31 or I don't know maybe not whales but\\n12:33 some other\\n12:35 bats.\\n12:37 Yeah. Yeah.\\n12:39 And the same uh uh the same thing u\\n12:42 lighter has um kind of similar idea but\\n12:46 it's instead of um a radio wave it uses\\n12:50 lasers, right?\\n12:52 Uh it it's light. Yes, that's right.\\n12:55 Like light rays. Yes. Okay.\\n12:57 That's why the lighter thing. Yeah.\\n12:59 Okay. I thought it's laser there. Okay.\\n13:01 Uh similar like I I think it's one of\\n13:03 the light frequencies.\\n13:05 Okay. Okay. And um\\n13:08 I don't know if you can disclose or talk\\n13:11 about things you do at work, but um I\\n13:13 know that these things they are used\\n13:16 often for cars, right? For self-driving.\\n13:21 Yeah. Uh I think it uh like depends on\\n13:23 the stack. Uh some companies they do use\\n13:28 like uh most of the like uh fully\\n13:31 self-driving where there is like\\n13:33 absolutely no driver use it. Uh whereas\\n13:36 if you see some of the Tesla systems,\\n13:38 they do not use it at all. They rely\\n13:40 solely on the cameras. Yeah.\\n13:41 Mhm. So Tesla I um took a few times a\\n13:46 taxi which turned out to be Tesla and\\n13:49 for me it was so fun to watch um they\\n13:52 have this uh screen and as the car\\n13:55 drives they start showing uh like cars\\n13:58 and people and uh stuff like around the\\n14:01 car, right? And uh for me it was always\\n14:04 cu\",\n",
       "  'video_id': 'vK_SxyqIfwk'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tools.search('Tesla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a97af8ce-3c0b-4a38-a69c-a8106ff7f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tools = SearchTools()\n",
    "\n",
    "search_tool_methods = [\n",
    "    function_tool(search_tools.search),\n",
    "    function_tool(search_tools.index_content)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56984946-31bd-4796-aca8-229bfb3dfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_instructions = \"\"\"\n",
    "Your task is to search through indexed documents.\n",
    "\n",
    "Before performing a search:\n",
    "1. Check if the document has been indexed.\n",
    "2. If not, call `index_content` to index it.\n",
    "\n",
    "Important rules:\n",
    "- Do NOT call `search` using a YouTube ID, webpage URL, or any direct link as the query.\n",
    "- After successfully indexing a source, do NOT perform a search automatically.\n",
    "- Instead, tell the user something like:\n",
    "  \"The video (or page) has been indexed. What would you like to learn about it?\"\n",
    "\n",
    "Only perform a search if the user provides a natural-language question or topic of interest\n",
    "(e.g., \"What does the video say about AI safety?\"), not a URL or ID.\n",
    "Do not attempt to summarize content.\n",
    "\"\"\"\n",
    "\n",
    "search_agent = Agent(\n",
    "    name='search_agent',\n",
    "    handoff_description='use this agent when you need to perform search in the knowledge base',\n",
    "    tools=search_tool_methods,\n",
    "    instructions=search_instructions,\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453b85b-d958-49f6-ae52-a124efff2e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
