{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464c8b20-f54c-4a89-ab49-afbcd25be06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                repo_owner: str,\n",
    "                repo_name: str,\n",
    "                allowed_extensions: Iterable[str] | None = None,\n",
    "                filename_filter: Callable[[str], bool] | None = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "        \n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "        \n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "            \n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "        \n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "            \n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3afd60-2826-459c-bcfb-5d2cec9ba48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_github_data():\n",
    "    allowed_extensions = {\"md\", \"mdx\"}\n",
    "\n",
    "    repo_owner = 'evidentlyai'\n",
    "    repo_name = 'docs'\n",
    "\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions\n",
    "    )\n",
    "    \n",
    "    return reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd69ade-b82c-45bb-9687-42ff0bd3c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 95 files\n"
     ]
    }
   ],
   "source": [
    "data_raw = read_github_data()\n",
    "print(f\"Downloaded {len(data_raw)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6399b7-0f2c-4d21-91d6-5aeb91e8f9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m152 packages\u001b[0m \u001b[2min 0.87ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m147 packages\u001b[0m \u001b[2min 0.99ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add python-frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af19aaf9-446a-4cb4-a4d9-8939ab1fe41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Evidently and GitHub actions\"\n",
      "description: \"Testing LLM outputs as part of the CI/CD flow.\"\n",
      "---\n",
      "\n",
      "You can use Evidently together with GitHub Actions to automatically test the outputs of your LLM agent or application - as part of every code push or pull request.\n",
      "\n",
      "## How the integration work:\n",
      "\n",
      "- You define a test dataset of inputs (e.g. test prompts with or without reference answers). You can store it as a file, or save the dataset at Evidently Cloud callable by Dataset ID.\n",
      "- Run your LLM system or agent against those inputs inside CI.\n",
      "- Evidently automatically evaluates the outputs using the user-specified config (which defines the Evidently descriptors, tests and Report composition), including methods like:\n",
      "  - LLM judges (e.g., tone, helpfulness, correctness)\n",
      "  - Custom Python functions\n",
      "  - Dataset-level metrics like classification quality\n",
      "- If any test fails, the CI job fails.\n",
      "- You get a detailed test report with pass/fail status and metrics.\n",
      "\n",
      "![](/images/examples/github_actions.gif)\n",
      "\n",
      "Results are stored locally or pushed to Evidently Cloud for deeper review and tracking.\n",
      "\n",
      "The final result is CI-native testing for your LLM behavior - so you can safely tweak prompts, models, or logic without breaking things silently.\n",
      "\n",
      "## Code example and tutorial\n",
      "\n",
      "ðŸ‘‰ Check the full tutorial and example repo: https://github.com/evidentlyai/evidently-ci-example\n",
      "\n",
      "Action is also available on GitHub Marketplace: https://github.com/marketplace/actions/run-evidently-report\n"
     ]
    }
   ],
   "source": [
    "print(data_raw[40].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f704dd13-b567-43f9-a85f-5b89dcad0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "def parse_data(data_raw):\n",
    "    \n",
    "\n",
    "    data_parsed = []\n",
    "    for f in data_raw:\n",
    "        post = frontmatter.loads(f.content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = f.filename\n",
    "        data_parsed.append(data)\n",
    "\n",
    "    return data_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "459902df-bb46-484b-a0cc-da11224aa7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = parse_data(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709b550b-0e6e-460a-8c09-d283543e6e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Evidently and GitHub actions',\n",
       " 'description': 'Testing LLM outputs as part of the CI/CD flow.',\n",
       " 'content': 'You can use Evidently together with GitHub Actions to automatically test the outputs of your LLM agent or application - as part of every code push or pull request.\\n\\n## How the integration work:\\n\\n- You define a test dataset of inputs (e.g. test prompts with or without reference answers). You can store it as a file, or save the dataset at Evidently Cloud callable by Dataset ID.\\n- Run your LLM system or agent against those inputs inside CI.\\n- Evidently automatically evaluates the outputs using the user-specified config (which defines the Evidently descriptors, tests and Report composition), including methods like:\\n  - LLM judges (e.g., tone, helpfulness, correctness)\\n  - Custom Python functions\\n  - Dataset-level metrics like classification quality\\n- If any test fails, the CI job fails.\\n- You get a detailed test report with pass/fail status and metrics.\\n\\n![](/images/examples/github_actions.gif)\\n\\nResults are stored locally or pushed to Evidently Cloud for deeper review and tracking.\\n\\nThe final result is CI-native testing for your LLM behavior - so you can safely tweak prompts, models, or logic without breaking things silently.\\n\\n## Code example and tutorial\\n\\nðŸ‘‰ Check the full tutorial and example repo: https://github.com/evidentlyai/evidently-ci-example\\n\\nAction is also available on GitHub Marketplace: https://github.com/marketplace/actions/run-evidently-report',\n",
       " 'filename': 'examples/GitHub_actions.mdx'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a89d3027-10fd-4560-a2cf-471367889a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can view or export Reports in multiple formats.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Log to Workspace\\n\\nYou can save the computed Report in Evidently Cloud or your local workspace.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n<Info>\\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\\n</Info>\\n\\n## View in Jupyter notebook\\n\\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\\n\\nAfter running the Report, simply call the resulting Python object:\\n\\n```python\\nmy_report\\n```\\n\\nThis will render the HTML object directly in the notebook cell.\\n\\n## HTML\\n\\nYou can also save this interactive visual Report as an HTML file to open in a browser:\\n\\n```python\\nmy_report.save_html(â€œfile.htmlâ€)\\n```\\n\\nThis option is useful for sharing Reports with others or if you\\'re working in a Python environment that doesnâ€™t display interactive visuals.\\n\\n## JSON\\n\\nYou can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\\n\\nTo view the JSON in Python:\\n\\n```python\\nmy_report.json()\\n```\\n\\nTo save the JSON as a separate file:\\n\\n```python\\nmy_report.save_json(\"file.json\")\\n```\\n\\n## Python dictionary\\n\\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\\n\\nTo get the dictionary:\\n\\n```python\\nmy_report.dict()\\n```'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data[10]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ad2908-1593-4aca-88e8-969d50358bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Document chunking utilities for splitting large documents into smaller, overlapping pieces.\n",
    "\n",
    "This module provides functionality to break down documents into chunks using a sliding\n",
    "window approach, which is useful for processing large texts in smaller, manageable pieces\n",
    "while maintaining context through overlapping content.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "\n",
    "def sliding_window(\n",
    "        seq: Iterable[Any],\n",
    "        size: int,\n",
    "        step: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (string or list) to be chunked.\n",
    "        size (int): The size of each chunk/window.\n",
    "        step (int): The step size between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "            - 'start': The starting position of the chunk in the original sequence\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If size or step are not positive integers.\n",
    "\n",
    "    Example:\n",
    "        >>> sliding_window(\"hello world\", size=5, step=3)\n",
    "        [{'start': 0, 'content': 'hello'}, {'start': 3, 'content': 'lo wo'}]\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "        documents: Iterable[Dict[str, str]],\n",
    "        size: int = 2000,\n",
    "        step: int = 1000,\n",
    "        content_field_name: str = 'content'\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split a collection of documents into smaller chunks using sliding windows.\n",
    "\n",
    "    Takes documents and breaks their content into overlapping chunks while preserving\n",
    "    all other document metadata (filename, etc.) in each chunk.\n",
    "\n",
    "    Args:\n",
    "        documents: An iterable of document dictionaries. Each document must have a content field.\n",
    "        size (int, optional): The maximum size of each chunk. Defaults to 2000.\n",
    "        step (int, optional): The step size between chunks. Defaults to 1000.\n",
    "        content_field_name (str, optional): The name of the field containing document content.\n",
    "                                          Defaults to 'content'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunk dictionaries. Each chunk contains:\n",
    "            - All original document fields except the content field\n",
    "            - 'start': Starting position of the chunk in original content\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Example:\n",
    "        >>> documents = [{'content': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, size=100, step=50)\n",
    "        >>> # Or with custom content field:\n",
    "        >>> documents = [{'text': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, content_field_name='text')\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop(content_field_name)\n",
    "        chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        results.extend(chunks)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ba2a4f-4c29-4830-9291-2c7990572606",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6598236d-d25e-4343-ab1d-0f54fb1af893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks[100]\n",
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f513bd-a217-4182-ba31-4c428efe2cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7498343116d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index= Index(\n",
    "    text_fields=[\"content\",\"filename\",\"title\",\"description\"],\n",
    ")\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66c52f25-70a7-4cc5-9366-2339f4d92735",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = index.search(\"How do I use llm-as-ajudge for evals?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "497ac9f7-9417-49eb-b410-4f75aaa84bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return index.search(query=query, num_results=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867fde36-4515-4460-9fce-24cd8390009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I use llm-as-ajudge for evals?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43d6060a-80ef-4223-be97-74ca9d8d8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documnentation. Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "Use only the facts from the CONTEXT when answering that QUESTION.\n",
    "When answering the question, provide the reference to the file with the source. Use the filename field for that. \n",
    "The repo url is: https://github.com/evidentlyai/docs/\n",
    "Include code examples when relevant.\n",
    "If the question is discussed in multiple documents, cite all ot them.\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template=\"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "252fb02e-f430-4699-b638-5290ee0638c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, search_result):\n",
    "    context = json.dumps(search_result)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dd128ab-bdad-477a-9d14-0aa53876175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75d7a6cc-1400-48e9-b2d9-80aea90ec2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(user_prompt,instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages=[]\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"instructions\"\n",
    "        })\n",
    "    messages.append({\n",
    "        \"role\":\"user\",\n",
    "        \"content\":user_prompt\n",
    "    })\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d235ac8-db41-49b3-8f91-e5555a30e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cb5e8e6-c164-4a0b-ad56-192c6e59b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag(query):\n",
    "    search_result = search(query)\n",
    "    prompt = build_prompt(query, search_result)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13819fbe-1b42-4a17-bcaf-fe206e4f3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ef8ec5a-f23d-41da-8978-50ee22d8a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use an LLM as a judge for evaluations, follow these steps:\n",
      "\n",
      "### 1. **Installation and Setup**\n",
      "First, install the required package:\n",
      "\n",
      "```bash\n",
      "pip install evidently\n",
      "```\n",
      "\n",
      "Next, import the necessary libraries:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from evidently import Dataset, DataDefinition, Report\n",
      "from evidently.presets import TextEvals\n",
      "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "```\n",
      "\n",
      "Don't forget to set your OpenAI API key:\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "```\n",
      "\n",
      "### 2. **Create the Dataset**\n",
      "Create a dataset that includes:\n",
      "- **Questions**: Inputs sent to the LLM.\n",
      "- **Target responses**: Approved responses considered accurate.\n",
      "- **New responses**: Responses generated from the system.\n",
      "- **Manual labels**: Labels indicating whether a response is correct.\n",
      "\n",
      "For example:\n",
      "\n",
      "```python\n",
      "data = {\n",
      "    'question': [\"What is the capital of France?\", \"Explain photosynthesis.\"],\n",
      "    'target_response': [\"Paris\", \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll.\"],\n",
      "    'new_response': [\"Paris, the capital city of France.\", \"Photosynthesis is a vital process in plants.\"],\n",
      "    'manual_labels': [1, 1]  # 1 for correct, 0 for incorrect\n",
      "}\n",
      "\n",
      "df = pd.DataFrame(data)\n",
      "eval_dataset = Dataset.from_pandas(df, data_definition=DataDefinition())\n",
      "```\n",
      "\n",
      "### 3. **Create the LLM Judge**\n",
      "Design an evaluator prompt using a `BinaryClassificationPromptTemplate`:\n",
      "\n",
      "```python\n",
      "verbosity = BinaryClassificationPromptTemplate(\n",
      "    criteria=\"\"\"Conciseness refers to the quality of being brief and to the point...\"\"\",\n",
      "    target_category=\"concise\",\n",
      "    non_target_category=\"verbose\",\n",
      "    uncertainty=\"unknown\",\n",
      "    include_reasoning=True,\n",
      "    pre_messages=[(\"system\", \"You are an expert text evaluator.\")]\n",
      ")\n",
      "```\n",
      "\n",
      "Add this template as a descriptor:\n",
      "\n",
      "```python\n",
      "eval_dataset.add_descriptors(descriptors=[\n",
      "    LLMEval(\"new_response\",\n",
      "            template=verbosity,\n",
      "            provider=\"openai\",\n",
      "            model=\"gpt-4o-mini\",\n",
      "            alias=\"Verbosity\")\n",
      "])\n",
      "```\n",
      "\n",
      "### 4. **Run the Report**\n",
      "Generate the evaluation report:\n",
      "\n",
      "```python\n",
      "report = Report([TextEvals()])\n",
      "my_eval = report.run(eval_dataset, None)\n",
      "```\n",
      "\n",
      "### 5. **View Results**\n",
      "You can access the evaluation results, or convert the dataset to a DataFrame:\n",
      "\n",
      "```python\n",
      "print(eval_dataset.as_dataframe())\n",
      "```\n",
      "\n",
      "### 6. **Iterate and Improve**\n",
      "If necessary, adjust the prompt template or try different models to find the best evaluator for your data.\n",
      "\n",
      "This framework allows you to evaluate LLM outputs systematically and can be integrated into workflows for continuous improvement.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851658b3-bda3-42bf-9309-dde1908cb84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
